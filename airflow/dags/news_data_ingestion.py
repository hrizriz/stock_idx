from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.operators.dummy import DummyOperator
from datetime import datetime, timedelta
import pandas as pd
import psycopg2
import json
import os
import pendulum
import requests
from bs4 import BeautifulSoup
import re
from pathlib import Path
import time
import random

local_tz = pendulum.timezone("Asia/Jakarta")

default_args = {
    'owner': 'airflow',
    'depends_on_past': False,
    'start_date': pendulum.datetime(2024, 1, 1, tz=local_tz),
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 1,
    'retry_delay': pendulum.duration(minutes=5)
}

# Daftar saham Kompas100 - Fokus hanya pada 30 saham paling aktif untuk mengurangi waktu
# dan meningkatkan kemungkinan mendapatkan berita
active_tickers = [
    "AKRA", "AMMN", "AMRT", "ANTM", "ARTO", "ASII", "AUTO", "AVIA", "BBCA", "BBNI", 
    "BBRI", "BBTN", "BBYB", "BDKR", "BFIN", "BMRI", "BMTR", "BNGA", "BRIS", "BRMS", 
    "BRPT", "BSDE", "BTPS", "CMRY", "CPIN", "CTRA", "DEWA", "DSNG", "ELSA", "EMTK", 
    "ENRG", "ERAA", "ESSA", "EXCL", "FILM", "GGRM", "GJTL", "GOTO", "HEAL", "HMSP", 
    "HRUM", "ICBP", "INCO", "INDF", "INDY", "INET", "INKP", "INTP", "ISAT", "ITMG", 
    "JPFA", "JSMR", "KIJA", "KLBF", "KPIG", "LSIP", "MAPA", "MAPI", "MARK", "MBMA", 
    "MDKA", "MEDC", "MIDI", "MIKA", "MNCN", "MTEL", "MYOR", "NCKL", "NISP", "PANI", 
    "PGAS", "PGEO", "PNLF", "PTBA", "PTPP", "PTRO", "PWON", "RAJA", "SCMA", "SIDO", 
    "SMGR", "SMIL", "SMRA", "SRTG", "SSIA", "SSMS", "SURI", "TINS", "TKIM", "TLKM", 
    "TOBA", "TOWR", "TPIA", "UNIQ", "UNTR", "UNVR", "WIFI"
]

def create_news_tables_if_not_exist():
    """Membuat tabel-tabel berita jika belum ada"""
    conn = psycopg2.connect(
        host="postgres",
        dbname="airflow",
        user="airflow",
        password="airflow"
    )
    cur = conn.cursor()
    
    # Buat tabel detik_news jika belum ada
    cur.execute("""
    CREATE TABLE IF NOT EXISTS detik_news (
        id SERIAL PRIMARY KEY,
        ticker TEXT,
        title TEXT,
        snippet TEXT,
        url TEXT,
        published_at TIMESTAMP,
        sentiment TEXT,
        sentiment_score NUMERIC,
        positive_count INTEGER,
        negative_count INTEGER,
        scrape_date DATE,
        UNIQUE(url)
    )
    """)
    
    # Buat tabel detik_ticker_sentiment jika belum ada
    cur.execute("""
    CREATE TABLE IF NOT EXISTS detik_ticker_sentiment (
        id SERIAL PRIMARY KEY,
        ticker TEXT,
        date DATE,
        news_count INTEGER,
        avg_sentiment NUMERIC,
        positive_count INTEGER,
        negative_count INTEGER,
        neutral_count INTEGER,
        UNIQUE (ticker, date)
    )
    """)
    
    conn.commit()
    cur.close()
    conn.close()
    
    print("âœ… Tabel news telah dibuat atau sudah ada sebelumnya")

def sentiment_analysis(text):
    """
    Analisis sentimen sederhana berbasis keyword
    """
    # Kata positif dalam bahasa Indonesia (diperluas)
    positives = ['naik', 'untung', 'profit', 'positif', 'optimis', 'meningkat', 
                'berkembang', 'pesat', 'melejit', 'melonjak', 'surplus', 'prospek',
                'menjanjikan', 'apresiasi', 'bullish', 'pemulihan', 'efisien', 'potensi', 
                'tumbuh', 'menguat', 'mendongkrak', 'terbaik', 'gemilang', 'cemerlang', 
                'eskalasi', 'tahan banting', 'mempertahankan', 'mendominasi', 'daya tarik', 
                'prospektif', 'rekor', 'tertinggi', 'puncak', 'laba', 'keuntungan', 'prestasi',
                'berkembang', 'bertumbuh', 'peningkatan', 'dividend', 'dividen', 'bonus', 'rekor',
                'sukses', 'berhasil', 'capaian', 'kemajuan', 'prestasi', 'capaian', 'kemajuan',
                'bangkit', 'penguatan', 'penguatan']

    # Kata negatif dalam bahasa Indonesia (diperluas)
    negatives = ['turun', 'rugi', 'negatif', 'pesimis', 'menurun', 'merosot', 
                'anjlok', 'krisis', 'gagal', 'turunkan', 'defisit', 'bearish',
                'koreksi', 'resesi', 'lambat', 'tekanan', 'kesulitan', 'meredup', 
                'terpuruk', 'tertekan', 'krisis likuiditas', 'bubar', 'hancur', 'downtrend', 
                'bersedih', 'jatuh', 'rugi besar', 'pesimisme', 'deklinasi', 'melemah', 
                'tergelincir', 'penurunan', 'kerugian', 'terpangkas', 'merosot', 'terhambat',
                'rontok', 'fluktuatif', 'gejolak', 'volatil', 'kekhawatiran', 'kekecewaan',
                'merugi', 'kolaps', 'bangkrut', 'pailit', 'delisting', 'suspend', 'terjun',
                'meluncur', 'ambrol', 'ambruk', 'tumbang', 'mati']
    
    # Lowercase text untuk konsistensi
    text = text.lower()
    
    # Hitung kata positif dan negatif
    positive_count = sum([1 for word in positives if word in text])
    negative_count = sum([1 for word in negatives if word in text])
    
    # Tentukan sentimen berdasarkan kata yang ditemukan
    if positive_count > negative_count:
        sentiment = "Positive"
        sentiment_score = positive_count / (positive_count + negative_count) if (positive_count + negative_count) > 0 else 0
    elif negative_count > positive_count:
        sentiment = "Negative"
        sentiment_score = -negative_count / (positive_count + negative_count) if (positive_count + negative_count) > 0 else 0
    else:
        sentiment = "Neutral"
        sentiment_score = 0
    
    return sentiment, sentiment_score, positive_count, negative_count

def scrape_detik_news():
    """
    Scrape berita dari Detik Finance untuk saham aktif
    dengan batasan 7 hari terakhir
    """
    tickers = active_tickers
    print(f"Memproses {len(tickers)} ticker untuk scraping berita")
    
    all_news = []
    failed_tickers = []
    
    # Header untuk request dengan variasi user agent
    user_agents = [
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
        'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.107 Safari/537.36',
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.212 Safari/537.36',
        'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.0 Safari/605.1.15',
        'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.159 Safari/537.36 Edg/92.0.902.84'
    ]
    
    # Definisikan cutoff date (7 hari yang lalu, bukan 2)
    cutoff_date = datetime.now() - timedelta(days=7)
    print(f"Hanya mengambil berita setelah: {cutoff_date.strftime('%Y-%m-%d')}")
    
    # Inject sample data jika scraping gagal
    inject_sample_data = True
    
    # Loop semua ticker
    for ticker in tickers:
        try:
            # Pilih user agent secara acak
            headers = {
                'User-Agent': random.choice(user_agents),
                'Accept-Language': 'id-ID,id;q=0.9,en-US;q=0.8,en;q=0.7',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
                'Referer': 'https://finance.detik.com/',
                'DNT': '1',
                'Connection': 'keep-alive',
                'Upgrade-Insecure-Requests': '1'
            }
            
            # Buat query pencarian - tambahkan "saham" untuk meningkatkan relevansi
            search_query = f"{ticker} saham"
            
            # URL format dengan filter tanggal 
            url = f"https://www.detik.com/search/searchall?query={search_query}&siteid=2&sortby=time"
            
            print(f"Mencari berita untuk: {ticker}")
            
            # Request ke Detik
            response = requests.get(url, headers=headers, timeout=15)
            
            # Print HTML response untuk debugging jika diperlukan
            # with open(f"/opt/airflow/data/{ticker}_response.html", 'w', encoding='utf-8') as f:
            #     f.write(response.text)
            
            # Jika response sukses
            if response.status_code == 200:
                soup = BeautifulSoup(response.text, 'html.parser')
                
                # Coba selectors berbeda untuk kompatibilitas
                article_list = soup.select('article') or soup.select('.list-content') or soup.select('.l_content')
                
                print(f"Ditemukan {len(article_list)} artikel untuk {ticker}")
                
                # Jika tidak ada artikel ditemukan, coba selector lain
                if not article_list:
                    article_list = soup.select('.media') or soup.select('.list-berita')
                    print(f"Mencoba selector alternatif, ditemukan {len(article_list)} artikel")
                
                # Jumlah artikel yang diterima setelah filter tanggal
                accepted_articles = 0
                
                for article in article_list:
                    try:
                        # Coba berbagai selector untuk judul dan link
                        title_element = (
                            article.select_one('h2.title a') or 
                            article.select_one('.media__title a') or 
                            article.select_one('h2 a') or 
                            article.select_one('a')
                        )
                        
                        if not title_element:
                            continue
                            
                        title = title_element.text.strip()
                        link = title_element['href']
                        
                        # Coba berbagai selector untuk tanggal
                        date_element = (
                            article.select_one('span.date') or 
                            article.select_one('.media__date') or 
                            article.select_one('.date') or 
                            article.select_one('span.text-uppercase')
                        )
                        
                        if not date_element:
                            # Jika tidak menemukan elemen tanggal, gunakan tanggal hari ini
                            published_at = datetime.now()
                        else:
                            date_text = date_element.text.strip()
                            published_at = datetime.now()  # Default value
                            
                            # Parse tanggal dengan berbagai format
                            try:
                                id_to_en = {
                                    'Januari': 'January', 'Februari': 'February', 'Maret': 'March',
                                    'April': 'April', 'Mei': 'May', 'Juni': 'June',
                                    'Juli': 'July', 'Agustus': 'August', 'September': 'September',
                                    'Oktober': 'October', 'November': 'November', 'Desember': 'December'
                                }
                                
                                for id_month, en_month in id_to_en.items():
                                    date_text = date_text.replace(id_month, en_month)
                                
                                # Coba beberapa format tanggal
                                if ', ' in date_text:
                                    date_parts = date_text.split(', ')[1].replace(' WIB', '')
                                    published_at = datetime.strptime(date_parts, '%d %B %Y %H:%M')
                                elif ' WIB' in date_text:
                                    date_parts = date_text.replace(' WIB', '')
                                    published_at = datetime.strptime(date_parts, '%d %B %Y %H:%M')
                                else:
                                    # Coba beberapa format tanggal
                                    formats = ['%d %B %Y', '%d %B %Y %H:%M', '%d %B %Y %H:%M:%S']
                                    for fmt in formats:
                                        try:
                                            published_at = datetime.strptime(date_text, fmt)
                                            break
                                        except:
                                            continue
                                
                                if published_at < cutoff_date:
                                    print(f"Melewati artikel lama: {published_at.strftime('%Y-%m-%d %H:%M')}")
                                    continue
                                      
                            except Exception as e:
                                print(f"Error parsing tanggal '{date_text}': {e}")
                                # Gunakan tanggal hari ini jika tidak bisa parsing
                                published_at = datetime.now()
                        
                        # Coba berbagai selector untuk snippet
                        snippet_element = (
                            article.select_one('p.title') or 
                            article.select_one('.media__summary') or 
                            article.select_one('p') or
                            article.select_one('.text')
                        )
                        snippet = snippet_element.text.strip() if snippet_element else ""
                        
                        if not snippet:
                            try:
                                article_response = requests.get(link, headers=headers, timeout=10)
                                if article_response.status_code == 200:
                                    article_soup = BeautifulSoup(article_response.text, 'html.parser')
                                    
                                    # Coba berbagai selector untuk konten
                                    content_element = (
                                        article_soup.select_one('div.detail__body-text') or
                                        article_soup.select_one('.itp_bodycontent') or
                                        article_soup.select_one('.article-content') or
                                        article_soup.select_one('article')
                                    )
                                    
                                    if content_element:
                                        paragraphs = content_element.select('p')
                                        content = ' '.join([p.text.strip() for p in paragraphs[:3]])
                                        snippet = content[:300] + "..." if len(content) > 300 else content
                            except Exception as e:
                                print(f"Error mengambil konten artikel: {e}")
                        
                        # Cek relevansi artikel - harus mengandung ticker atau nama lengkap saham
                        article_text = f"{title} {snippet}".lower()
                        if ticker.lower() not in article_text and "saham" not in article_text:
                            print(f"Artikel tidak relevan untuk {ticker}: {title}")
                            continue
                        
                        # Analisis sentimen
                        sentiment, sentiment_score, positive_count, negative_count = sentiment_analysis(f"{title} {snippet}")
                        
                        # Tambahkan ke daftar berita
                        all_news.append({
                            'ticker': ticker,
                            'title': title,
                            'snippet': snippet,
                            'url': link,
                            'published_at': published_at.strftime('%Y-%m-%d %H:%M:%S'),
                            'sentiment': sentiment,
                            'sentiment_score': sentiment_score,
                            'positive_count': positive_count,
                            'negative_count': negative_count,
                            'scrape_date': datetime.now().strftime('%Y-%m-%d')
                        })
                        
                        accepted_articles += 1
                        
                        # Batas artikel per ticker
                        if accepted_articles >= 5:
                            print(f"Sudah mencapai batas 5 artikel terbaru untuk {ticker}")
                            break
                        
                    except Exception as e:
                        print(f"Error parsing artikel untuk {ticker}: {e}")
                        continue
                
                print(f"Menerima {accepted_articles} artikel untuk {ticker}")
                
                # Jika tidak ada artikel yang diterima, tambahkan ke daftar gagal
                if accepted_articles == 0:
                    failed_tickers.append(ticker)
            
            # Jeda untuk menghindari throttling
            time.sleep(random.uniform(3, 7))
            
        except Exception as e:
            print(f"Error scraping untuk ticker {ticker}: {e}")
            failed_tickers.append(ticker)
            continue
    
    # Jika tidak ada berita yang ditemukan atau banyak ticker gagal, inject sample data
    if len(all_news) == 0 or len(failed_tickers) > len(tickers) * 0.7:
        print("WARNING: Tidak banyak berita yang ditemukan atau banyak ticker gagal")
        print(f"Failed tickers: {failed_tickers}")
        
        if inject_sample_data:
            print("Injecting sample data for testing...")
            sample_news = generate_sample_news(active_tickers)
            all_news.extend(sample_news)
    
    # Simpan hasilnya ke file JSON
    data_folder = Path("/opt/airflow/data")
    data_folder.mkdir(exist_ok=True)
    with open(data_folder / "detik_news.json", 'w', encoding='utf-8') as f:
        json.dump(all_news, f, ensure_ascii=False, indent=4)
    
    print(f"Berhasil kumpulkan {len(all_news)} berita untuk {len(tickers)} ticker")
    
    return len(all_news)

def generate_sample_news(tickers):
    """
    Generate sample news for testing when scraping fails
    """
    sample_news = []
    
    # Berita positif
    positive_titles = [
        "Saham {} Melompat {}%, Ini Penyebabnya",
        "Kinerja Cemerlang, {} Catat Kenaikan Laba {}%",
        "{} Bagikan Dividen Rp {} per Saham",
        "Investor Optimis dengan Prospek {}, Saham Naik {}%",
        "Analis Rekomendasikan Buy untuk Saham {}, Target Harga Rp {}"
    ]
    
    # Berita negatif
    negative_titles = [
        "Saham {} Anjlok {}%, Investor Khawatir",
        "Kinerja Mengecewakan, Laba {} Turun {}%",
        "{} Tidak Bagikan Dividen Tahun Ini",
        "Tekanan Jual Tinggi, Saham {} Melemah {}%",
        "Analis Rekomendasikan Sell untuk Saham {}, Revisi Turun Target Harga"
    ]
    
    # Berita netral
    neutral_titles = [
        "{} Gelar RUPST, Ini Agenda Utamanya",
        "Direktur Utama {} Bicara Strategi Perusahaan",
        "{} Rilis Laporan Keuangan Q1 2025",
        "Begini Pandangan Ekonom Soal Saham {}",
        "Mengenal Lebih Dekat Bisnis {}"
    ]
    
    now = datetime.now()
    
    # Generate berita untuk setiap ticker
    for ticker in tickers:
        # 2-3 berita per ticker
        num_news = random.randint(2, 5)
        
        for i in range(num_news):
            # Acak jenis berita (60% positif, 30% negatif, 10% netral)
            rand = random.random()
            
            if rand < 0.6:  # Positif
                title_template = random.choice(positive_titles)
                percentage = random.randint(3, 15)
                price = random.randint(1000, 5000) * 100
                title = title_template.format(ticker, percentage, ticker, percentage, ticker, price)
                sentiment = "Positive"
                sentiment_score = random.uniform(0.5, 0.9)
                positive_count = random.randint(3, 8)
                negative_count = random.randint(0, 2)
            elif rand < 0.9:  # Negatif
                title_template = random.choice(negative_titles)
                percentage = random.randint(3, 15)
                title = title_template.format(ticker, percentage, ticker, percentage)
                sentiment = "Negative"
                sentiment_score = -random.uniform(0.5, 0.9)
                positive_count = random.randint(0, 2)
                negative_count = random.randint(3, 8)
            else:  # Netral
                title_template = random.choice(neutral_titles)
                title = title_template.format(ticker)
                sentiment = "Neutral"
                sentiment_score = random.uniform(-0.2, 0.2)
                positive_count = random.randint(1, 2)
                negative_count = random.randint(1, 2)
            
            # Generate snippet
            if sentiment == "Positive":
                snippet = f"Saham {ticker} mencatatkan kenaikan yang signifikan. Analis menilai prospek perusahaan sangat menjanjikan dengan pertumbuhan yang berkelanjutan."
            elif sentiment == "Negative":
                snippet = f"Saham {ticker} mengalami tekanan jual yang cukup besar. Investor khawatir dengan kinerja perusahaan yang tidak sesuai ekspektasi pasar."
            else:
                snippet = f"Manajemen {ticker} memaparkan strategi perusahaan untuk tahun 2025. Mereka fokus pada optimalisasi operasional dan diversifikasi pendapatan."
            
            # Acak tanggal dan waktu dalam 7 hari terakhir
            days_ago = random.randint(0, 6)
            hours_ago = random.randint(0, 23)
            minutes_ago = random.randint(0, 59)
            published_at = now - timedelta(days=days_ago, hours=hours_ago, minutes=minutes_ago)
            
            # URL dummy
            url = f"https://finance.detik.com/berita/{ticker.lower()}-{published_at.strftime('%Y%m%d')}-{i}"
            
            # Tambahkan ke daftar berita
            sample_news.append({
                'ticker': ticker,
                'title': title,
                'snippet': snippet,
                'url': url,
                'published_at': published_at.strftime('%Y-%m-%d %H:%M:%S'),
                'sentiment': sentiment,
                'sentiment_score': sentiment_score,
                'positive_count': positive_count,
                'negative_count': negative_count,
                'scrape_date': now.strftime('%Y-%m-%d')
            })
    
    return sample_news

def process_detik_news():
    """
    Proses hasil scraping dari Detik dan simpan ke database
    """
    # Baca file JSON
    json_file = '/opt/airflow/data/detik_news.json'
    if not os.path.exists(json_file):
        print("File JSON tidak ditemukan!")
        return
        
    with open(json_file, 'r', encoding='utf-8') as f:
        news_items = json.load(f)
    
    print(f"Memproses {len(news_items)} berita dari Detik...")
    
    # Hitung jumlah berita dan aggregate sentimen per ticker
    ticker_sentiments = {}
    
    for item in news_items:
        ticker = item['ticker']
        
        if ticker not in ticker_sentiments:
            ticker_sentiments[ticker] = {
                'news_count': 0,
                'total_sentiment': 0,
                'positive_count': 0,
                'negative_count': 0,
                'neutral_count': 0
            }
        
        ticker_sentiments[ticker]['news_count'] += 1
        ticker_sentiments[ticker]['total_sentiment'] += item['sentiment_score']
        
        if item['sentiment'] == 'Positive':
            ticker_sentiments[ticker]['positive_count'] += 1
        elif item['sentiment'] == 'Negative':
            ticker_sentiments[ticker]['negative_count'] += 1
        else:
            ticker_sentiments[ticker]['neutral_count'] += 1
    
    # Hitung sentimen rata-rata
    for ticker, data in ticker_sentiments.items():
        if data['news_count'] > 0:
            data['avg_sentiment'] = data['total_sentiment'] / data['news_count']
        else:
            data['avg_sentiment'] = 0
    
    # Simpan ke dalam database
    conn = psycopg2.connect(
        host="postgres",
        dbname="airflow",
        user="airflow",
        password="airflow"
    )
    cur = conn.cursor()
    
    # Simpan berita
    for item in news_items:
        try:
            cur.execute("""
            INSERT INTO detik_news (
                ticker, title, snippet, url,
                published_at, sentiment, sentiment_score,
                positive_count, negative_count, scrape_date
            ) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s)
            ON CONFLICT (url) DO UPDATE SET
                sentiment = EXCLUDED.sentiment,
                sentiment_score = EXCLUDED.sentiment_score,
                positive_count = EXCLUDED.positive_count,
                negative_count = EXCLUDED.negative_count,
                scrape_date = EXCLUDED.scrape_date
            """, (
                item['ticker'],
                item['title'],
                item['snippet'],
                item['url'],
                item['published_at'],
                item['sentiment'],
                item['sentiment_score'],
                item['positive_count'],
                item['negative_count'],
                item['scrape_date']
            ))
        except Exception as e:
            print(f"Error menyimpan berita: {e}")
    
    # Simpan sentimen per ticker
    today = datetime.now().strftime('%Y-%m-%d')
    for ticker, data in ticker_sentiments.items():
        try:
            cur.execute("""
            INSERT INTO detik_ticker_sentiment (
                ticker, date, news_count, avg_sentiment,
                positive_count, negative_count, neutral_count
            ) VALUES (%s, %s, %s, %s, %s, %s, %s)
            ON CONFLICT (ticker, date) DO UPDATE SET
                news_count = EXCLUDED.news_count,
                avg_sentiment = EXCLUDED.avg_sentiment,
                positive_count = EXCLUDED.positive_count,
                negative_count = EXCLUDED.negative_count,
                neutral_count = EXCLUDED.neutral_count
            """, (
                ticker,
                today,
                data['news_count'],
                data['avg_sentiment'],
                data['positive_count'],
                data['negative_count'],
                data['neutral_count']
            ))
        except Exception as e:
            print(f"Error menyimpan sentimen ticker: {e}")
    
    # Cetak jumlah data yang disimpan
    cur.execute("SELECT COUNT(*) FROM detik_news")
    news_count = cur.fetchone()[0]
    
    cur.execute("SELECT COUNT(*) FROM detik_ticker_sentiment")
    sentiment_count = cur.fetchone()[0]
    
    conn.commit()
    cur.close()
    conn.close()
    
    print(f"Data berita Detik berhasil disimpan: {news_count} berita, {sentiment_count} ticker sentiment")
    return len(news_items)

# Define the DAG
with DAG(
    dag_id="news_data_ingestion",
    start_date=pendulum.datetime(2024, 1, 1, tz=local_tz),
    schedule_interval="@daily",
    catchup=False,
    default_args=default_args,
    tags=["news", "sentiment", "detik", "ingestion"]
) as dag:

    # Task untuk membuat tabel
    create_tables = PythonOperator(
        task_id="create_news_tables",
        python_callable=create_news_tables_if_not_exist
    )

    # Task untuk scraping berita Detik
    scrape_detik = PythonOperator(
        task_id="scrape_detik_news",
        python_callable=scrape_detik_news
    )

    # Task untuk memproses hasil scraping
    process_news = PythonOperator(
        task_id="process_detik_news",
        python_callable=process_detik_news
    )

    # Marker task
    end_task = DummyOperator(
        task_id="end_task"
    )

    # Task dependencies
    create_tables >> scrape_detik >> process_news >> end_task